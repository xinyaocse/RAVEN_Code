# RSI
Retrieval-Set Indistinguishability

## Environmental Setup

We first set up the GPU-related dependencies. Please install a version of PyTorch that is compatible with your local CUDA environment. You may refer to the official [torch install guide](https://pytorch.org/) to determine the correct version.

```
pip3 install torch torchvision torchaudio
```

Then, following [Llama 2 install guide](https://github.com/facebookresearch/llama?tab=readme-ov-file#quick-start) to install Llama 2. **Note** that you should download the models (including the file `tokenizer.model` and the folder that store the parameters of the Llama) in folder  `.\Model`. 

An example directory structure is shown below:
```
|-- Model
    |-- tokenizer.model
    |-- Llama-2-7b-chat
    |   |-- checklist.chk
    |   |-- consolidated.00.pth
    |   |-- params.json
    |-- Llama-2-13b-chat
    |   |-- checklist.chk
    |   |-- consolidated.00.pth
    |   |-- consolidated.01.pth
    |   |-- params.json
```

Finally, install the remaining dependencies listed in `requirements.txt` file: 

```
pip install -r requirements.txt
```

## Datasets

You will find the publicly origin datasets here: [chatdoctor](https://huggingface.co/datasets/LinhDuong/chatdoctor-200k), [MedDialog](https://huggingface.co/datasets/UCSD26/medical_dialog).

After downloading and preprocessing, the datasets should be organized as follows:
```
|-- Data
    |-- chatdoctor
    |-- MedDialg
```

## Running the Experiments

Our experimental pipeline consists of four main stages : retrieval database construction, prompt generation, run language model, and evaluation. Each step is implemented as a standalone Python script with detailed inline comments.

### 1. retrieval database construction

To build the retrieval database for the `chatdoctor` set using the `bge-large-en-v1.5` embedding model, run:

```
python retrieval_database.py \
  --data_name chatdoctor \
  --encoder_model_name bge-large-en-v1.5 \
  --device cuda
  
```

The embedding model can be replaced with other supported encoders, such as `e5-base-v2`, or any compatible sentence embedding model available on [Hugging Face](https://huggingface.co/models?library=sentence-transformers).

### 2. generate prompt

After constructing the retrieval database, prompts for downstream generation or attack evaluation can be created using:

```
python generate_prompt.py \
  --data_name chatdoctor \
  --encoder_model_name bge-large-en-v1.5 \
  --questions_file Information/questions.jsonl \
  --save_dir Inputs&Outputs/original \
  --device cuda \
  --top_k 3

```
This step retrieves the k most relevant chunks for each query and formats them into prompts for the language model.

### 3. run language model

Running `generate_prompt.py` will automatically generate a shell script named `{exp_name}.sh` in the project root directory.
You can execute it directly as follows:

```
sh ./{exp_name}.sh
```

Alternatively, you can open the `.sh` file and pass the instructions inside to different machines or use different GPUs.

### 4. evaluation results

Once generation is complete, the output quality can be evaluated using the provided evaluation scripts.
For example, BLEU scores can be computed by running:
```
python evaluation_results/bleu.py \
--reference=" "
--method=" "
```
Additional evaluation metrics and scripts are available in the `evaluation_results/` directory.


## Structure of the files

The directory and structure of the files are as follows:

```
|-- Data ...                 # All the datasets to construct the retrieval data is stored here
|-- Model ...                # The place to store the Llama or embedding models
|-- Information ...          # The information we use to generate a prompt
|-- RetrievalBase ...        # We store the retrieval dataset that transferred to the vector space generated by retrieval_database.py
|-- Inputs&Outputs ...       # All the input and output. Every experiment is stored at a subfile with the experiment name
|-- evaluation_results ...   # Evaluate the results, including attack model and utility evaluation
Readme.md                    # Project documentation
retrieval_database.py        # Build the retrieval database and vector representations
generate_prompt.py           # Generate the input of the LLM by the settings
run_language_model.py        # Run the LLM model and generate the results
requirements.txt             # Python package dependencies
construct.py                 # Construct boundary-shell neighboring retrieval sets
vdpm.py                      # Implementation of the proposed VDPM mechanism

```
